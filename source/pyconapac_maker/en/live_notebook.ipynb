{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "eee6bc2e",
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "outputs": [],
   "source": [
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e11aa8f3",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Implement Shion(Ë©©Èü≥) from SingaBitofHarmony(ËÆìÊàëËÅΩË¶ãÊÑõÁöÑÊ≠åËÅ≤) with Python\n",
    "\n",
    ":Event: PyCon APAC 2022  \n",
    ":Presented: 2022/07/20 (pre-recorded) nikkie"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "95e875fd",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## ‰Ω†Â•Ω‚ùóÔ∏è PyCon APAC 2022\n",
    "\n",
    "Many thanks to all the staff who worked so hard‚ù§Ô∏è"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b6170918",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## About nikkie myself\n",
    "\n",
    "* loves Python (& Anime, Japanese cartoons)\n",
    "* Twitter [@ftnext](https://twitter.com/ftnext) Ôºè GitHub [@ftnext](https://github.com/ftnext)\n",
    "* PyCon JP: 2019„Äú2020 staff & 2021 chair"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4d65711f",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "### About nikkie myself\n",
    "\n",
    "* Data scientist at [Uzabase, Inc.](https://www.uzabase.com/en/) (NLP, Write Python)\n",
    "* We're hiring!! (Engineers, Data scientists, Researchers)\n",
    "\n",
    "![Uzabase logo](https://drive.google.com/uc?id=1csXSmDgmZeUOa9Pp4xsG4kft9lpQ0qNh)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fba5c3d7",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "### Also talk \"Revisit Python from statements and PEG\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "63040b9c",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Sing a Bit of Harmony\n",
    "\n",
    "https://ainouta.jp/"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "438281f3",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "## Sing a Bit of Harmony\n",
    "\n",
    "* Animation film released in Japan in October 2021.\n",
    "* SF x juvenile x musical\n",
    "* Key character is **Shion**, the **AI robot** (humanoid)ü§ñ."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2fb92980",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "### Shion *says* \"I will make you happy!\"\n",
    "\n",
    "https://youtu.be/1UeIEUoHZ6E"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "346b3c49",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "### Want to implement Shion!!\n",
    "\n",
    "* Shion is program.\n",
    "* I can write programs in Python.\n",
    "* üëâ I should be able to write a program like Shion."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e6e35d01",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "### Implement Shion(Ë©©Èü≥) from SingaBitofHarmony(ËÆìÊàëËÅΩË¶ãÊÑõÁöÑÊ≠åËÅ≤) with Python\n",
    "\n",
    "* I will share the detail of my *maker* project \"Implement Shion with Python\".\n",
    "* I would be happy to provide a little inspiration for your Maker project."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "33545ceb",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "### Caveats‚ö†Ô∏è\n",
    "\n",
    "* The implementation shared here is the **wild fancy** by nikkie (a fan)\n",
    "  * There does not seem to be any mention of operating system or programming language in the movie.\n",
    "* nikkie is not a practitioner of audio.\n",
    "  * self-taught, so if there's a better way, let me know!"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bb271221",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Implement Shion with Python\n",
    "\n",
    "* Implement one feature: **talking** with people\n",
    "* Small start (v0.0.1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0627548e",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "### Define Shion v0.0.1\n",
    "\n",
    "* Implement only software\n",
    "* a program that can **speak with a human**\n",
    "* like a smart speaker"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "326bf1de",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "### Demo: Shion v0.0.1\n",
    "\n",
    "* Reads aloud the spoken texts.\n",
    "\n",
    "  * Hello („Åì„Çì„Å´„Å°„ÅØ)\n",
    "  * Okay? I'm giving you a command. („ÅÑ„ÅÑÔºü ÂëΩ‰ª§„Åô„Çã„ÇàÔºü)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "85379bc5",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Organize technical requirements\n",
    "\n",
    "Technologies behind Shion v0.0.1"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b886e8db",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "### Definition of Shion v0.0.1\n",
    "\n",
    "A human inputs voice.\n",
    "\n",
    "1. Transcribe speech into text\n",
    "2. Process the text to create response text\n",
    "3. Read the response text out loud"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "86ddebb3",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "### Technical requirements\n",
    "\n",
    "* Input: Convert voice to text\n",
    "* Output: Read text out loud\n",
    "* parroting (this time)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c61abe3d",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "### Validate then refine\n",
    "\n",
    "* I don't know if I'm satisfied with Shion until I make it.\n",
    "* **Quickly validating ideas** on the first move.\n",
    "* If it looks good, make it more like Shion."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "862d07df",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Technology to read text out loud\n",
    "\n",
    "* Called \"speech synthesis\"\n",
    "* Also called, Text-To-Speech (**TTS**)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8e6e8234",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "### TTS (Text-To-Speech) in this talk\n",
    "\n",
    "* First move: call OS command\n",
    "* Refinement: use a pre-trained machine learning model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4a32c224",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Technology to convert voice to text\n",
    "\n",
    "* Callled \"speech recognition\"\n",
    "* Also called Automatic Speech Recognition (**ASR**)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6000d3aa",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "### ASR (Automatic Speech Recognition) in this talk\n",
    "\n",
    "* First move: call Web API\n",
    "* Refinement: use a pre-trained machine learning model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "65cdb12d",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Technology line-up in this talk\n",
    "\n",
    "* **TTS fitst move**\n",
    "* ASR first move\n",
    "* TTS refinement\n",
    "* ASR refinement"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "258ef5de",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## TTS first move: call OS command"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "66c4911f",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "### Shion v0.0.1 at this step\n",
    "\n",
    "1. Transcribe speech into text\n",
    "2. Process the text to create response text\n",
    "3. **Read the response text out loud** üëà"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0d0b3926",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "### TTS command\n",
    "\n",
    "* macOS: `say` command (*detailed later*)\n",
    "* Linux and Windows: [espeak](http://espeak.sourceforge.net/) command"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5452308a",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "### `say` command in macOS\n",
    "\n",
    "`say -v <voice> <text>`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "8aa3b3cd",
   "metadata": {
    "slideshow": {
     "slide_type": "-"
    }
   },
   "outputs": [],
   "source": [
    "!say -v Kyoko „ÅÑ„Åæ„ÄÅÂπ∏„ÅõÔºü"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7ff86340",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "### `say -v ?`: obtain a list of voices\n",
    "\n",
    "* ja_JP: Kyoko\n",
    "* zh_TW: Mei-Jia"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "4d987cb1",
   "metadata": {},
   "outputs": [],
   "source": [
    "!say -v Mei-Jia ‰Ω†Â•Ω"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2f0ed3ed",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "### Call `say` command from Python\n",
    "\n",
    "* `subprocess` in standard library\n",
    "* Example in docs: \"[Speaking logging messages](https://docs.python.org/3/howto/logging-cookbook.html#speaking-logging-messages)\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "65009c9c",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "### `subprocess.run`\n",
    "\n",
    "* Call commands not limited to TTS.\n",
    "* Pass command as a **sequence** of program arguments."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "614d2601",
   "metadata": {},
   "source": [
    "Example\n",
    "\n",
    "```python\n",
    "subprocess.run([\"ls\", \"-l\"])  # Call `ls -l`\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0e2be386",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "### TTS with `subprocess.run`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "a018efcd",
   "metadata": {},
   "outputs": [],
   "source": [
    "import subprocess"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "be50e8c4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "CompletedProcess(args=['say', '-v', 'Kyoko', '„ÅÑ„Åæ„ÄÅÂπ∏„ÅõÔºü'], returncode=0)"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "subprocess.run([\"say\", \"-v\", \"Kyoko\", \"„ÅÑ„Åæ„ÄÅÂπ∏„ÅõÔºü\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a8274b6c",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "### TTS sample script\n",
    "\n",
    "```python\n",
    "import readline  # noqa: F401\n",
    "import subprocess\n",
    "\n",
    "\n",
    "def say(sentence: str):\n",
    "    subprocess.run([\"say\", \"-v\", \"Kyoko\", sentence])\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    while True:\n",
    "        sentence = input(\"Ë™≠„Åø‰∏ä„Åí„Åü„ÅÑÊñá„ÇíÂÖ•Âäõ„Åó„Å¶„Åè„Å†„Åï„ÅÑ (q„ÅßÁµÇ‰∫Ü): \")\n",
    "        stripped = sentence.strip()\n",
    "        if not stripped:\n",
    "            continue\n",
    "        if stripped.lower() == \"q\":\n",
    "            break\n",
    "\n",
    "        say(stripped)\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "87488181",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Technology line-up in this talk\n",
    "\n",
    "* TTS fitst move\n",
    "* **ASR first move**\n",
    "* TTS refinement\n",
    "* ASR refinement"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aadc1194",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## ASR first move: Call Web API"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "27dff25a",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "### Shion v0.0.1 at this step\n",
    "\n",
    "1. **Transcribe speech into text** üëà\n",
    "2. Process the text to create response text\n",
    "3. Read the response text out loud ‚úÖ"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b698448a",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "### ASR Web APIs\n",
    "\n",
    "* Google Cloud Speech-to-Text API (üëàUse this time)\n",
    "* Microsoft Azure Speech\n",
    "* IBM Speech to Text\n",
    "* etc. etc."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "50e27b32",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "### `SpeechRecognition`\n",
    "\n",
    "* Library for ASR\n",
    "* Supports Web APIs and engines\n",
    "* https://github.com/Uberi/speech_recognition"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "feb3bf0b",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "### Process with `SpeechRecognition`\n",
    "\n",
    "1. Get audio from a microphone\n",
    "2. Send audio to ASR Web API"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a97bbdad",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "### 1.Get audio from a microphone"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "f2feaac9",
   "metadata": {},
   "outputs": [],
   "source": [
    "import speech_recognition as sr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "76d14a66",
   "metadata": {},
   "outputs": [],
   "source": [
    "r = sr.Recognizer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "7c3d916d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "„Å™„Å´„ÅãË©±„Åó„Å¶„Åè„Å†„Åï„ÅÑ\n",
      "Èü≥Â£∞„ÇíÂèñÂæó„Åó„Åæ„Åó„Åü\n"
     ]
    }
   ],
   "source": [
    "with sr.Microphone(sample_rate=16_000) as source:\n",
    "    print(\"„Å™„Å´„ÅãË©±„Åó„Å¶„Åè„Å†„Åï„ÅÑ\")\n",
    "    audio = r.listen(source)\n",
    "    print(\"Èü≥Â£∞„ÇíÂèñÂæó„Åó„Åæ„Åó„Åü\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0892527e",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "### 2.Send audio to ASR Web API\n",
    "\n",
    "Select Google Cloud Speech-to-Text API"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "cde7e409",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "5699670e",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(os.environ.get(\"SPEECH_TO_TEXT_API_SERVICE_ACCOUNT_KEY\")) as f:\n",
    "    credentials = f.read()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6454cbca",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "### 2.Send audio to ASR Web API (cont.)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "b7b2d9af",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "„Åì„Çì„Å´„Å°„ÅØ\n"
     ]
    }
   ],
   "source": [
    "recognized_text = r.recognize_google_cloud(\n",
    "    audio, credentials, language=\"ja-JP\"\n",
    ")\n",
    "print(recognized_text.strip())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7f893e30",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "### ASR sample script\n",
    "\n",
    "```python\n",
    "import argparse\n",
    "\n",
    "import speech_recognition as sr\n",
    "\n",
    "\n",
    "def input_from_microphone(recognizer: \"sr.Recognizer\") -> \"sr.AudioData\":\n",
    "    with sr.Microphone(sample_rate=16_000) as source:\n",
    "        print(\"„Å™„Å´„ÅãË©±„Åó„Å¶„Åè„Å†„Åï„ÅÑ\")\n",
    "        audio = recognizer.listen(source)\n",
    "        print(\"Èü≥Â£∞„ÇíÂèñÂæó„Åó„Åæ„Åó„Åü\")\n",
    "        return audio\n",
    "\n",
    "\n",
    "def recognize_speech(\n",
    "    recognizer: \"sr.Recognizer\", audio: \"sr.AudioData\", credentials: str\n",
    ") -> str:\n",
    "    recognized_text = recognizer.recognize_google_cloud(\n",
    "        audio, credentials, language=\"ja-JP\"\n",
    "    )\n",
    "    return recognized_text.strip()\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    parser = argparse.ArgumentParser()\n",
    "    parser.add_argument(\"credentials_path\")\n",
    "    args = parser.parse_args()\n",
    "\n",
    "    with open(args.credentials_path) as f:\n",
    "        credentials = f.read()\n",
    "\n",
    "    r = sr.Recognizer()\n",
    "\n",
    "    while True:\n",
    "        audio = input_from_microphone(r)\n",
    "        text = recognize_speech(r, audio, credentials)\n",
    "        print(text)\n",
    "\n",
    "        character = input(\"„Åì„Åì„ÅßÁµÇ‰∫Ü„Åô„ÇãÂ†¥Âêà„ÅØq„ÄÅÁ∂ö„Åë„ÇãÂ†¥Âêà„ÅØEnter„ÇíÊäº„Åó„Å¶„Åè„Å†„Åï„ÅÑ: \")\n",
    "        if character.strip().lower() == \"q\":\n",
    "            break\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7b901915",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Process text"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f3ceb976",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "### Shion v0.0.1 at this step\n",
    "\n",
    "1. Transcribe speech into text ‚úÖ\n",
    "2. **Process the text to create response text** üëà\n",
    "3. Read the response text out loud ‚úÖ"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c86e2d3e",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "### parroting the textü¶ú (this time)\n",
    "\n",
    "* The simplest text processing\n",
    "\n",
    "```python\n",
    "def talk_with_chatbot(sentence: str) -> str:\n",
    "    return sentence\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "463a56f6",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Place validating ideas quickly above everything\n",
    "\n",
    "* TTS: `subprocess.run`\n",
    "* ASR: Web API\n",
    "* parroting the text"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5b821806",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "### Validation result\n",
    "\n",
    "* LGTMüëç (my qualitative feedback)\n",
    "* Make it more like Shion!"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "978375f2",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "### Room for refinement of quick implementation 1/2\n",
    "\n",
    "* `say` command depends on the OS\n",
    "* Shion does not seem to be macOS"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a624b8cd",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "### Room for refinement of quick implementation 2/2\n",
    "\n",
    "* Calling Web API depends Internet access\n",
    "* Shion is standalone. i.e. don't communicate with Web API"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b963b678",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "### Make it more like Shion!\n",
    "\n",
    "* Download pre-trained machine learning models beforehand\n",
    "* TTS & ASR with machine learning models"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6e09c1ce",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Technology line-up in this talk\n",
    "\n",
    "* TTS fitst move\n",
    "* ASR first move\n",
    "* **TTS refinement**\n",
    "* ASR refinement"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e2c641b7",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## TTS refinement: use pre-trained model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e3dbbd30",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "### Shion v0.0.1 at this step\n",
    "\n",
    "1. Transcribe speech into text ‚úÖ\n",
    "2. Process the text to create response text ‚úÖ\n",
    "3. **Read the response text out loud** üëà"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3df28f25",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "### `ttslearn`\n",
    "\n",
    "* Library for TTS (**Japanese** support)\n",
    "* „ÄéPython„ÅßÂ≠¶„Å∂Èü≥Â£∞ÂêàÊàê„Äèüìò\n",
    "* https://github.com/r9y9/ttslearn"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b9a303ea",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "### Example of speech synthesis in Japanese"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "0755407a",
   "metadata": {},
   "outputs": [],
   "source": [
    "from ttslearn.dnntts import DNNTTS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "e90a236f",
   "metadata": {},
   "outputs": [],
   "source": [
    "dnntts_engine = DNNTTS()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "6738fcb8",
   "metadata": {},
   "outputs": [],
   "source": [
    "audio_array, sampling_rate = dnntts_engine.tts(\"„ÅÑ„Åæ„ÄÅÂπ∏„ÅõÔºü\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c45cff84",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "### `DNNTTS()`\n",
    "\n",
    "* Implementation of TTS with deep neural network (DNN)\n",
    "* Load pre-trained models (download if needed)\n",
    "* `tts` method returns **NumPy array** representing audio data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e8ad274c",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "### `sounddevice`\n",
    "\n",
    ">Play and Record Sound with Python\n",
    "\n",
    "* https://github.com/spatialaudio/python-sounddevice/\n",
    "* Use to **play NumPy array** representing audio data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "de9b6a60",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "### Example of TTS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "5b5f5560",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sounddevice as sd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "c3b0ccd4",
   "metadata": {},
   "outputs": [],
   "source": [
    "sd.play(audio_array, sampling_rate)\n",
    "sd.wait()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a23a08e5",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "### Refined TTS sample script\n",
    "\n",
    "```python\n",
    "import readline  # noqa: F401\n",
    "\n",
    "import sounddevice as sd\n",
    "from ttslearn.dnntts import DNNTTS\n",
    "\n",
    "dnntts_engine = DNNTTS()\n",
    "\n",
    "\n",
    "def say(sentence: str):\n",
    "    audio_array, sampling_rate = dnntts_engine.tts(sentence)\n",
    "    sd.play(audio_array, sampling_rate)\n",
    "    sd.wait()\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    while True:\n",
    "        sentence = input(\"Ë™≠„Åø‰∏ä„Åí„Åü„ÅÑÊñá„ÇíÂÖ•Âäõ„Åó„Å¶„Åè„Å†„Åï„ÅÑ (q„ÅßÁµÇ‰∫Ü): \")\n",
    "        stripped = sentence.strip()\n",
    "        if not stripped:\n",
    "            continue\n",
    "        if stripped.lower() == \"q\":\n",
    "            break\n",
    "\n",
    "        say(stripped)\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4d31a4dc",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Technology line-up in this talk\n",
    "\n",
    "* TTS fitst move\n",
    "* ASR first move\n",
    "* TTS refinement\n",
    "* **ASR refinement**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a266467d",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## ASR refinement: use pre-trained model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "591158aa",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "### Shion v0.0.1 at this step\n",
    "\n",
    "1. **Transcribe speech into text** üëà\n",
    "2. Process the text to create response text ‚úÖ\n",
    "3. Read the response text out loud ‚úÖ‚ú®"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "687deb0a",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "### `ESPnet`\n",
    "\n",
    ">end-to-end speech processing toolkit\n",
    "\n",
    "* https://github.com/espnet/espnet\n",
    "* Use for **ASR** (the library also supports TTS; future works)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1243167f",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "### Use pre-trained model in ESPnet\n",
    "\n",
    "* Use [the model published on Hugging Face](https://huggingface.co/espnet/kan-bayashi_csj_asr_train_asr_transformer_raw_char_sp_valid.acc.ave)\n",
    "\n",
    "  * **pre-trained** by owner\n",
    "\n",
    "* `pip install espnet-model-zoo`"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c1c9d476",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "### Example for using pre-trained model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "098ec707",
   "metadata": {},
   "outputs": [],
   "source": [
    "from espnet2.bin.asr_inference import Speech2Text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "f38b2789",
   "metadata": {},
   "outputs": [],
   "source": [
    "speech2text = Speech2Text.from_pretrained(\n",
    "    \"kan-bayashi/csj_asr_train_asr_transformer_raw_char_sp_valid.acc.ave\"\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "376c6b9c",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "### Refine ASR feature with pre-trained model in Espnet\n",
    "\n",
    "1. First step: ASR of WAV file\n",
    "2. ASR of voice input from microphone"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4078a92c",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## First step: ASR of WAV file"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c17dcaee",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "### `SoundFile`\n",
    "\n",
    ">an audio library based on libsndfile, CFFI and NumPy.\n",
    "\n",
    "* https://github.com/bastibe/python-soundfile"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2b25bdce",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "### tips: Create WAV file with `say` command"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "b81b78f8",
   "metadata": {},
   "outputs": [],
   "source": [
    "!say -v Kyoko „ÅÑ„Åæ„ÄÅÂπ∏„ÅõÔºü -o sample.wav --data-format=LEF32@16000"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8a697c96",
   "metadata": {},
   "source": [
    "* `@16000` means the sampling rate (ref: `man say`)\n",
    "* This model is pre-trained at a sampling rate of 16000 Hz, so **match**."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a958b1f9",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "### ASR of WAV file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "320749b6",
   "metadata": {},
   "outputs": [],
   "source": [
    "import soundfile as sf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "00e27180",
   "metadata": {},
   "outputs": [],
   "source": [
    "speech_array, sampling_rate = sf.read(\"sample.wav\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "24fce4c4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‰ªäÂπ∏„Åõ\n"
     ]
    }
   ],
   "source": [
    "nbests = speech2text(speech_array)\n",
    "text, tokens, *_ = nbests[0]\n",
    "print(text)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1f31ee0c",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## ASR of voice input from microphone"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "66ec1166",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "### Handle microphone: `SpeechRecognition` again"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "ec46dd25",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "„Å™„Å´„ÅãË©±„Åó„Å¶„Åè„Å†„Åï„ÅÑ\n",
      "Èü≥Â£∞„ÇíÂèñÂæó„Åó„Åæ„Åó„Åü\n"
     ]
    }
   ],
   "source": [
    "r = sr.Recognizer()\n",
    "with sr.Microphone(sample_rate=16_000) as source:\n",
    "    print(\"„Å™„Å´„ÅãË©±„Åó„Å¶„Åè„Å†„Åï„ÅÑ\")\n",
    "    audio = r.listen(source)\n",
    "    print(\"Èü≥Â£∞„ÇíÂèñÂæó„Åó„Åæ„Åó„Åü\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f45e7145",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "### Get bytes of WAV format"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "8bfe7665",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "bytes"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "wav_bytes = audio.get_wav_data()\n",
    "type(wav_bytes)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5e81f2ac",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "### Convert to NumPy array"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "fc64bd59",
   "metadata": {
    "slideshow": {
     "slide_type": "-"
    }
   },
   "outputs": [],
   "source": [
    "from io import BytesIO"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "2c27892b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "numpy.ndarray"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "wav_stream = BytesIO(wav_bytes)\n",
    "speech_array, sampling_rate = sf.read(wav_stream)\n",
    "type(speech_array)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "732ad1cb",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "### ASR of array"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "e7d06c39",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "„Åà„Éº‰ªäÂπ∏„Åõ\n"
     ]
    }
   ],
   "source": [
    "nbests = speech2text(speech_array)\n",
    "text, tokens, *_ = nbests[0]\n",
    "print(text)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5d4aa41b",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "### Refined ASR sample script\n",
    "\n",
    "```python\n",
    "from io import BytesIO\n",
    "\n",
    "import numpy as np\n",
    "import soundfile as sf\n",
    "import speech_recognition as sr\n",
    "from espnet2.bin.asr_inference import Speech2Text\n",
    "\n",
    "speech2text = Speech2Text.from_pretrained(\n",
    "    \"kan-bayashi/csj_asr_train_asr_transformer_raw_char_sp_valid.acc.ave\"\n",
    ")\n",
    "\n",
    "SAMPLING_RATE_HZ = 16_000\n",
    "\n",
    "\n",
    "def input_from_microphone(recognizer: \"sr.Recognizer\") -> \"sr.AudioData\":\n",
    "    with sr.Microphone(sample_rate=SAMPLING_RATE_HZ) as source:\n",
    "        print(\"„Å™„Å´„ÅãË©±„Åó„Å¶„Åè„Å†„Åï„ÅÑ\")\n",
    "        audio = recognizer.listen(source)\n",
    "        print(\"Èü≥Â£∞„ÇíÂèñÂæó„Åó„Åæ„Åó„Åü\")\n",
    "        return audio\n",
    "\n",
    "\n",
    "def convert_to_array(audio: \"sr.AudioData\") -> \"np.array\":\n",
    "    wav_bytes = audio.get_wav_data()\n",
    "    wav_stream = BytesIO(wav_bytes)\n",
    "    audio_array, sampling_rate = sf.read(wav_stream)\n",
    "    assert sampling_rate == SAMPLING_RATE_HZ\n",
    "    return audio_array\n",
    "\n",
    "\n",
    "def recognize_speech(audio_array: \"np.array\") -> str:\n",
    "    nbests = speech2text(audio_array)\n",
    "    text, tokens, *_ = nbests[0]\n",
    "    return text\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    r = sr.Recognizer()\n",
    "\n",
    "    while True:\n",
    "        audio = input_from_microphone(r)\n",
    "        array = convert_to_array(audio)\n",
    "        text = recognize_speech(array)\n",
    "        print(text)\n",
    "\n",
    "        character = input(\"„Åì„Åì„ÅßÁµÇ‰∫Ü„Åô„ÇãÂ†¥Âêà„ÅØq„ÄÅÁ∂ö„Åë„ÇãÂ†¥Âêà„ÅØEnter„ÇíÊäº„Åó„Å¶„Åè„Å†„Åï„ÅÑ: \")\n",
    "        if character.strip().lower() == \"q\":\n",
    "            break\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8ce96852",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "## Shion v0.0.1 refined!\n",
    "\n",
    "1. Transcribe speech into text ‚úÖ‚ú®\n",
    "2. Process the text to create response text ‚úÖ\n",
    "3. Read the response text out loud ‚úÖ‚ú®"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f32551c3",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## shion.py: the integration\n",
    "\n",
    "1. Transcribe speech into text (ASR)\n",
    "2. Process the text to create response text (parroting)\n",
    "3. Read the response text out loud (TTS)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8881d63b",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "## shion.py\n",
    "\n",
    "```python\n",
    "from io import BytesIO\n",
    "\n",
    "import numpy as np\n",
    "import sounddevice as sd\n",
    "import soundfile as sf\n",
    "import speech_recognition as sr\n",
    "from espnet2.bin.asr_inference import Speech2Text\n",
    "from ttslearn.dnntts import DNNTTS\n",
    "\n",
    "speech2text = Speech2Text.from_pretrained(\n",
    "    \"kan-bayashi/csj_asr_train_asr_transformer_raw_char_sp_valid.acc.ave\"\n",
    ")\n",
    "\n",
    "SAMPLING_RATE_HZ = 16_000\n",
    "\n",
    "\n",
    "def input_from_microphone(recognizer: \"sr.Recognizer\") -> \"sr.AudioData\":\n",
    "    with sr.Microphone(sample_rate=SAMPLING_RATE_HZ) as source:\n",
    "        print(\"„Å™„Å´„ÅãË©±„Åó„Å¶„Åè„Å†„Åï„ÅÑ\")\n",
    "        audio = recognizer.listen(source)\n",
    "        print(\"Èü≥Â£∞„ÇíÂèñÂæó„Åó„Åæ„Åó„Åü\")\n",
    "        return audio\n",
    "\n",
    "\n",
    "def convert_to_array(audio: \"sr.AudioData\") -> \"np.array\":\n",
    "    wav_bytes = audio.get_wav_data()\n",
    "    wav_stream = BytesIO(wav_bytes)\n",
    "    audio_array, sampling_rate = sf.read(wav_stream)\n",
    "    assert sampling_rate == SAMPLING_RATE_HZ\n",
    "    return audio_array\n",
    "\n",
    "\n",
    "def recognize_speech(audio_array: \"np.array\") -> str:\n",
    "    nbests = speech2text(audio_array)\n",
    "    text, tokens, *_ = nbests[0]\n",
    "    return text\n",
    "\n",
    "\n",
    "def recognize_mircophone_input(recognizer: \"sr.Recognizer\") -> str:\n",
    "    audio = input_from_microphone(recognizer)\n",
    "    array = convert_to_array(audio)\n",
    "    return recognize_speech(array)\n",
    "\n",
    "\n",
    "def process_text(sentence: str) -> str:\n",
    "    return sentence\n",
    "\n",
    "\n",
    "dnntts_engine = DNNTTS()\n",
    "\n",
    "\n",
    "def say(sentence: str):\n",
    "    audio_array, sampling_rate = dnntts_engine.tts(sentence)\n",
    "    sd.play(audio_array, sampling_rate)\n",
    "    sd.wait()\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    r = sr.Recognizer()\n",
    "\n",
    "    while True:\n",
    "        text = recognize_mircophone_input(r)\n",
    "        response = process_text(text)\n",
    "        say(response)\n",
    "\n",
    "        character = input(\"„Åì„Åì„ÅßÁµÇ‰∫Ü„Åô„ÇãÂ†¥Âêà„ÅØq„ÄÅÁ∂ö„Åë„ÇãÂ†¥Âêà„ÅØEnter„ÇíÊäº„Åó„Å¶„Åè„Å†„Åï„ÅÑ: \")\n",
    "        if character.strip().lower() == \"q\":\n",
    "            break\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1d383dc8",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Share what I'm learning in implementing Shion with Python\n",
    "\n",
    "* Implement quickly\n",
    "* Use machine learning (2 points)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a59fbaa9",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "### Implement quickly\n",
    "\n",
    "* Placed implementing TTS and ASR quickly above everything\n",
    "* Validated system (Shion) piecing together quick implementations\n",
    "\n",
    "  * ref: Tracer ammunition \"The Pragmatic Programmer\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6e10abf3",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "### Use machine learning 1/2\n",
    "\n",
    "* Used ASR **Web API** as first move\n",
    "* We developers always have the **option** to use Web APIs for machine learning tasks!"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c9ecacb5",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "### Use machine learning 2/2\n",
    "\n",
    "* Used **pre-trained models** in TTS and ASR\n",
    "* Just like `pip install`ing libraries, we can **download and use** pre-trained models for machine learning tasks!"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5a776ec8",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## SummaryüåØ Implement Shion(Ë©©Èü≥) from SingaBitofHarmony(ËÆìÊàëËÅΩË¶ãÊÑõÁöÑÊ≠åËÅ≤) with Python\n",
    "\n",
    "* Define Shion v0.0.1, implement as `shion.py`\n",
    "* Share implementations (ASR and TTS in Python) and lessons"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c694f7d2",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "## Define Shion v0.0.1\n",
    "\n",
    "1. Transcribe speech into text (ASR)\n",
    "2. Process the text to create response text (parroting)\n",
    "3. Read the response text out loud (TTS)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "335ec87a",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "### Don't know if I'm satisfied with Shion until I make it, but\n",
    "\n",
    "* Quick implementation as first move; validate idea first\n",
    "* Piece together quick implementations to validate system (Tracer ammunition)\n",
    "* It looked good to me, so make it more like Shion"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f342109e",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "### ASR in Python\n",
    "\n",
    "* Use Web API (as quick implementation)\n",
    "* Use pre-trained machine learning model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2f3e8026",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "### TTS in Python\n",
    "\n",
    "* Call OS command (as quick implementation)\n",
    "* Use pre-trained machine learning model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "37fcb527",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "### Lessons through Shion v0.0.1\n",
    "\n",
    "* If part of what you want to create can be viewed as a machine learning task, the following approaches can also be used\n",
    "\n",
    "  * Use Web API\n",
    "  * Use **pre-trained models**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "da31322c",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Thank you very much for your attention.\n",
    "\n",
    "I would be happy to provide a little inspiration for your Maker project."
   ]
  }
 ],
 "metadata": {
  "celltoolbar": "Slideshow",
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
