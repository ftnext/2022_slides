ASR 作り込み：機械学習モデル利用
============================================================

ライブラリ ``ESPnet``
--------------------------------------------------

* エンドツーエンドの音声処理ツールキット
* **音声認識（ASR）を利用** （TTSもサポート）
* https://github.com/espnet/espnet

事前訓練済みモデルの利用
--------------------------------------------------

* `Hugging Faceで公開されているモデル <https://huggingface.co/espnet/kan-bayashi_csj_asr_train_asr_transformer_raw_char_sp_valid.acc.ave>`_ を利用

  * 作成者により **事前訓練済み** （pre-trained）

* :command:`pip install espnet-model-zoo`

事前訓練済みモデルを利用するコード例
--------------------------------------------------

.. code-block:: python

    >>> from espnet2.bin.asr_inference import Speech2Text
    >>> speech2text = Speech2Text.from_pretrained(
    ...     "kan-bayashi/csj_asr_train_asr_transformer_raw_char_sp_valid.acc.ave"
    ... )

Espnetの事前訓練済みモデルでASR作り込み
--------------------------------------------------

1. First step: wavファイルのASR
2. マイクから入力した音声のASR

First step: wavファイルのASR
============================================================

ライブラリ ``SoundFile``
--------------------------------------------------

* 音声のライブラリ

    an audio library based on libsndfile, CFFI and NumPy.

* https://github.com/bastibe/python-soundfile

wavファイルのASR
--------------------------------------------------

.. code-block:: python

    >>> import soundfile as sf
    >>> speech_array, sampling_rate = sf.read("sample.wav")
    >>> nbests = speech2text(speech_array)
    >>> text, tokens, *_ = nbests[0]
    >>> print(text)
    今幸せ

tips: :command:`say` コマンドでwavファイルを作れる！
------------------------------------------------------------

.. code-block:: shell

    $ say -v Kyoko いま、幸せ？ -o sample.wav --data-format=LEF32@16000

* ``@`` 以降がサンプリングレート（:command:`man say` 参照）
* 今回のモデルはサンプリングレート16000Hzで訓練されているので **合わせる**

マイクから入力した音声のASR
============================================================

* マイクの操作には ``SpeechRecognition`` を使います

.. code-block:: python

    >>> r = sr.Recognizer()
    >>> with sr.Microphone(sample_rate=16_000) as source:
    ...     audio = r.listen(source)

NumPy arrayに変換
--------------------------------------------------

.. code-block:: python

    >>> import numpy as np
    >>> frame_bytes = audio.get_raw_data()
    >>> speech_array = np.frombuffer(frame_bytes, dtype=np.int16)

落とし穴：arrayのdtype
--------------------------------------------------

* wavファイルを ``sounddevice`` で読み込むと ``dtype`` は **float64**
* マイクから入力した音声をarrayに変換したところ、 ``dtype`` は *int16*
* 訓練済みモデルは dtype=int16 のデータに対して実力を発揮しきれない

dtypeをint16からfloat64に変換
--------------------------------------------------

.. code-block:: python

    >>> import tempfile
    >>> from scipy.io import wavfile
    >>> with tempfile.NamedTemporaryFile() as tempf:
    ...     wavfile.write(tempf.name, audio.sample_rate, speech_array)
    ...     audio_array, sampling_rate = sf.read(tempf.name)

dtype float64のarrayについてASR
--------------------------------------------------

.. code-block:: python

    >>> nbests = speech2text(audio_array)
    >>> text, tokens, *_ = nbests[0]
    >>> print(text)

作り込んだASRサンプルスクリプト
--------------------------------------------------

.. literalinclude:: ../../samplecode/my_shion/asr_pretrained_model.py
    :language: python
    :linenos:
