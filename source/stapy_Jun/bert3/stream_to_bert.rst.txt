ニューラル言語モデル
============================================================

* *ニューラルネットワーク* により実現される
* *言語モデル*

ニューラルネットワーク（📚[2] 2-2）
--------------------------------------------------

* 機械学習アルゴリズムの1つ
* **変換を行う層** を（何層も） **組合せる**

.. あたらしいデータ分析の教科書

言語モデル（📚[2] 2-1）
--------------------------------------------------

* 文章の **出現しやすさ** を確率によってモデル化

  * 「私はパンを食べた」>「私はパンに食べた」>「私は家を食べた」

* この確率は文章の **自然さ** とも見なせる

ニューラル言語モデルの訓練
--------------------------------------------------

* 言語モデルとなるニューラルネットワークを訓練する
* 自然言語から **ラベル付きデータを自動で作れる**

  * 例：それまでの単語たちから次の単語を予測

ニューラルネットワークへの入力
--------------------------------------------------

* 単語を **整数ID** に変換だけして入力

  * 例： ``7184, 30046, 9, 6040, 12, 31, 8``

* 各単語（ID）をどのようなベクトルとして扱うかも自動で見つける

用語紹介「深層学習」（📚[2] 1-3）
--------------------------------------------------

* 1つのモデルで特徴量抽出とその後の処理を行う
* **特徴量の抽出も含めて**、データからルールを **自動** で見つける
* 人手によらない特徴量抽出

BERT
============================================================

ようやくBERTの話ができる！（ゼイゼイ）

Transformer（📚[2] 3-1）
--------------------------------------------------

* 2017年の論文「`Attention is All You Need <https://papers.nips.cc/paper/2017/hash/3f5ee243547dee91fbd053c1c4a845aa-Abstract.html>`_」で提案されたモデル
* BERTはTransformerで提案されたニューラルネットワークを用いる

BERT
--------------------------------------------------

* 2018年発表、Bidirectional Encoder Representations from Transformersの略
* BERTもニューラル言語モデル
* **1つのモデルで複数のタスクを扱える** のが革新的

余談：BERTは *コンビニ*
--------------------------------------------------

* `NLP2022チュートリアル <https://www.slideshare.net/techblogyahoo/ss-251672433/90>`_ で聞いた喩え
* BERTは汎用モデルで、かつタスク固有のモデルを凌いだ
* BERT（コンビニの鰻・天ぷら）／タスク固有モデル（鰻屋の鰻・天ぷら屋の天ぷら）

ライブラリ `transformers` 🤗
--------------------------------------------------

* Transformerやそれ以後に派生したモデルを扱うためのライブラリ
* Transformerモデルの **訓練** や、 **読み込んで使う** のをサポート
* もちろんBERTもサポート
