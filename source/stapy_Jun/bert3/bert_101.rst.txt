BERTの訓練を押さえよう
============================================================

例を見たBERTについて **理論面を少しだけ**

これだけ押さえる： **転移学習**
--------------------------------------------------

訓練が2段階ある

* 事前学習（pre-train）
* ファインチューニング（fine-tuning）

事前学習
============================================================

* 大量のテキストを用意（日本語で書かれたWikipedia）
* （簡略化して言うと）テキスト中の単語を **穴抜け** にして、その単語を推測させて訓練

  * 穴抜けにすることで、大量の教師データを確保

.. PyCon JP 2021はBERTを将棋の盤面に使った例

.. self-attentionの話もしたい

事前学習
--------------------------------------------------

* 計算コストはめちゃかかる（一例： `TPUを使って5日間 <https://huggingface.co/cl-tohoku/bert-base-japanese-v2>`_）
* **誰かが一回** やればOK
* 事前学習済みのモデルがトークナイザと一緒に **公開** されている（例：東北大学乾研究室）

IMO：事前学習済みのモデルのイメージ
--------------------------------------------------

* 「こいつ、日本語わかってるな」
* 分かっているように見える例はすぐ後で

ファインチューニング
============================================================

* 事前学習済みモデルを **手元のデータで再訓練**
* 解きたいタスク向けにチューニングするイメージ

ファインチューニング
--------------------------------------------------

* 解きたいタスク用のデータ（**少量** でよい）を使って各自やる
* 計算コストはそんなにかからない（Colabを使う例を紹介）
* ファインチューニング済みのモデルも公開されている（感情分析の例で利用済み）
